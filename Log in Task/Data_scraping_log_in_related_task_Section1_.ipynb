{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data scraping - log in related task - Section1 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnZMj8JNRZ3q",
        "colab_type": "text"
      },
      "source": [
        "## Data scarping - log in task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4sNJ8u3axLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make some preparation for packages\n",
        "# package use\n",
        "import requests\n",
        "import urllib\n",
        "import urllib.request\n",
        "import time\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import copy\n",
        "import datetime \n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_kMFRPChcvn",
        "colab_type": "code",
        "outputId": "88e49017-649b-46f2-86bf-c0b16f4dce20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install mechanize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mechanize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/68/5ab6f96dfbdae2182e214cf9cc4790f2810c695c04034cc067b0e4ffa2b7/mechanize-0.4.3-py2.py3-none-any.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: html5lib>=0.999999999 in /usr/local/lib/python3.6/dist-packages (from mechanize) (1.0.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from html5lib>=0.999999999->mechanize) (1.12.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib>=0.999999999->mechanize) (0.5.1)\n",
            "Installing collected packages: mechanize\n",
            "Successfully installed mechanize-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8TTtBjguT0i",
        "colab_type": "code",
        "outputId": "83721786-3bed-4526-e16c-b0a6b796d277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install selenium"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDFkWwKtvhqT",
        "colab_type": "code",
        "outputId": "c6eec4b8-8da0-4687-fbd5-e6df64694e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/4b/585e8e111ffb01466c59281f34febb13ad1a95d7fb3919fd57c33fc732a5/Scrapy-1.7.3-py2.py3-none-any.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 4.3MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
            "Collecting queuelib (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting Twisted>=13.1.0; python_version != \"3.4\" (from scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/49/eb654da38b15285d1f594933eefff36ce03106356197dba28ee8f5721a79/Twisted-19.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 31.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.12.0)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting pyOpenSSL (from scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 20.1MB/s \n",
            "\u001b[?25hCollecting service-identity (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Collecting parsel>=1.5 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
            "Collecting incremental>=16.10.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting zope.interface>=4.4.2 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/17/1d198a6aaa9aa4590862fe3d3a2ed7dd808050cab4eebe8a2f2f813c1376/zope.interface-4.6.0-cp36-cp36m-manylinux1_x86_64.whl (167kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 34.9MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest>=1.9.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 18.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (19.1.0)\n",
            "Collecting Automat>=0.3.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Collecting cryptography>=2.3 (from pyOpenSSL->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.2.6)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.4.2->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (41.2.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (2.8)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.3->pyOpenSSL->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 27.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3->pyOpenSSL->scrapy) (1.12.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3->pyOpenSSL->scrapy) (2.19)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11516 sha256=e54fb86ea8d8b0af49b83e0398678bd99efddac9699739785d6e95601ccefa38\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: cssselect, w3lib, queuelib, incremental, zope.interface, constantly, PyHamcrest, Automat, hyperlink, Twisted, PyDispatcher, asn1crypto, cryptography, pyOpenSSL, service-identity, parsel, scrapy\n",
            "Successfully installed Automat-0.7.0 PyDispatcher-2.0.5 PyHamcrest-1.9.0 Twisted-19.7.0 asn1crypto-0.24.0 constantly-15.1.0 cryptography-2.7 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 parsel-1.5.2 pyOpenSSL-19.0.0 queuelib-1.5.0 scrapy-1.7.3 service-identity-18.1.0 w3lib-1.21.0 zope.interface-4.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSeSPjVHugbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "import time\n",
        "import scrapy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOlfrPw9aKMl",
        "colab_type": "text"
      },
      "source": [
        "## Task 1:\n",
        "\n",
        "(1) \"Film-masterfile (from 'film-merge-8th.dta').csv\"\n",
        "Here you can add the following variables.\n",
        " - 'releasedate': release date of each film\n",
        " - 'closedate': closing date of each film\n",
        " - 'numberofcritics': number of critics rating each film  \n",
        "\n",
        "You can use 'bomlink' to scrape variables 'releasedate' and 'closedate'.\n",
        "Given columns are already created in the csv file at the right side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSyCetIdRNBt",
        "colab_type": "code",
        "outputId": "b62410fe-0265-487d-8849-50a2acab5469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "# read the csv file first\n",
        "film_merge = pd.read_csv(\"Film-masterfile (from 'film-merge-8th.dta').csv\")\n",
        "print(film_merge.columns)\n",
        "\n",
        "# analyze bomlink column\n",
        "film_merge['bomlink'].head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['year', 'bomrank', 'bomtitle', 'bomlink', 'domesticgross',\n",
            "       'foreigngross', 'runtimemin', 'productionbudget', 'majordistributor',\n",
            "       'grated', 'rrated', 'unrated', 'remove', 'imdblink', 'studio',\n",
            "       'totalusgross', 'totaltheater', 'openingusgross', 'openingtheaters',\n",
            "       'foreigndm', 'languagecount', '_merge2', 'qualityfilm', 'countaward',\n",
            "       '_merge3', 'genrespanning', '_merge4', 'criticratings_avg',\n",
            "       'criticrating_var', 'user_avg', 'user_var', '_merge5', 'imdbfilmtitle',\n",
            "       'avg_minority_gender', 'avg_minority_race', 'avg_minority',\n",
            "       'avg_minority_1', 'multidirector', 'releasedate', 'closedate',\n",
            "       'numberofcritics'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    http://www.boxofficemojo.com/movies/?id=wizard...\n",
              "1    http://www.boxofficemojo.com/movies/?id=singin...\n",
              "2    http://www.boxofficemojo.com/movies/?id=raider...\n",
              "3    http://www.boxofficemojo.com/movies/?id=ghostb...\n",
              "4    http://www.boxofficemojo.com/movies/?id=backto...\n",
              "Name: bomlink, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHI3YBKbgfhQ",
        "colab_type": "text"
      },
      "source": [
        "### Task 1.1  Fill in the release date first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEe7J7U1Sabz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function to extract release date\n",
        "def extractRelease(bomlink_, keyword_ = \"date\"):\n",
        "  # set the url to the website and access the site\n",
        "  response = requests.get(bomlink_)\n",
        "  # parse the html with BeautifulSoup for data structure\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  # search for href link within central table\n",
        "  filter_ = soup.find(\"center\").findAll(\"a\")\n",
        "  for record_ in filter_:\n",
        "    if keyword_ in record_[\"href\"]:\n",
        "      # find release date record and cut\n",
        "      return record_.text\n",
        "  # records not found\n",
        "  return np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVRGQVJCZa8E",
        "colab_type": "code",
        "outputId": "eb173c34-8f8b-4c90-c6d4-4939140180d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# fill in release date \n",
        "# and turn time string into timestamp\n",
        "for i in range(0, film_merge.shape[0]):\n",
        "  film_merge['releasedate'][i] = pd.to_datetime(extractRelease(film_merge['bomlink'][i]), \n",
        "                                                infer_datetime_format=True)\n",
        "  # track process\n",
        "  if i%500 == 0:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAniQl4lWPLB",
        "colab_type": "code",
        "outputId": "86828137-50c3-446d-e1e1-00c17224fc6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "film_merge['releasedate'].head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2013-09-20 00:00:00\n",
              "1    2012-07-12 00:00:00\n",
              "2    2012-09-07 00:00:00\n",
              "3    2014-08-29 00:00:00\n",
              "4    2015-10-21 00:00:00\n",
              "5    2013-02-08 00:00:00\n",
              "6    2012-01-13 00:00:00\n",
              "7    2013-04-05 00:00:00\n",
              "8    2006-10-20 00:00:00\n",
              "9    2011-09-16 00:00:00\n",
              "Name: releasedate, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWwbpEfyfGwS",
        "colab_type": "text"
      },
      "source": [
        "### Task 1.2 Fill in the close date "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdjW5Fbidqtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# there are three tables in general\n",
        "# Total Lifetime Grosses - Domestic Summary - Charts\n",
        "def extractClose(bomlink_, keyword_ = \"Close Date:\"):\n",
        "  response = requests.get(bomlink_)\n",
        "  # parse the html with BeautifulSoup for data structure\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  # filter div box content to use \n",
        "  filter_ = soup.findAll(\"div\", attrs={\"class\": \"mp_box_content\"})\n",
        "  # assuming each page owns at least two tables \n",
        "  for i in range(0, len(filter_[1].findAll(\"td\"))):\n",
        "      # check available tables within 2nd box content\n",
        "      if filter_[1].findAll(\"td\")[i].get_text().replace('\\xa0',' ') == keyword_:\n",
        "        # Close date exists then return next string \n",
        "        date = filter_[1].findAll(\"td\")[i+1].get_text().replace('\\xa0',' ')\n",
        "        return pd.to_datetime(date, infer_datetime_format=True)\n",
        "  # Close date not exist\n",
        "  return np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyU5JsTcdqrP",
        "colab_type": "code",
        "outputId": "dae7db8f-b66c-480f-a385-f330f775c1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# fill in Close date \n",
        "for i in range(0, film_merge.shape[0]):\n",
        "  film_merge['closedate'][i] = extractClose(film_merge['bomlink'][i])\n",
        "  # track process\n",
        "  if i%500 == 0:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tspx_cQjmtt",
        "colab_type": "code",
        "outputId": "c08c2fbe-b592-4ff3-ad4a-fc5500e6f1ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "film_merge['closedate'].head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2013-10-17 00:00:00\n",
              "1    2012-08-26 00:00:00\n",
              "2    2012-10-04 00:00:00\n",
              "3    2014-09-18 00:00:00\n",
              "4    2015-10-21 00:00:00\n",
              "5    2013-02-21 00:00:00\n",
              "6    2012-05-03 00:00:00\n",
              "7    2013-05-23 00:00:00\n",
              "8    2007-01-04 00:00:00\n",
              "9    2012-01-12 00:00:00\n",
              "Name: closedate, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1FCchR27PJj",
        "colab_type": "code",
        "outputId": "f8697d65-eca0-441f-dcfa-b6acf0ccbc96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "# check the number of non-time records in release date and close date\n",
        "print(len(film_merge[film_merge['releasedate'].isnull()]))\n",
        "print(\"Number of films that lack close date:\", len(film_merge[film_merge['closedate'].isnull()]))\n",
        "film_merge[film_merge['closedate'].isnull()]['bomlink'].head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Number of films that lack close date: 43\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51     http://www.boxofficemojo.com/movies/?id=bobby.htm\n",
              "58     http://www.boxofficemojo.com/movies/?id=bluebu...\n",
              "114    http://www.boxofficemojo.com/movies/?id=santav...\n",
              "145    http://www.boxofficemojo.com/movies/?id=mainho...\n",
              "268    http://www.boxofficemojo.com/movies/?id=choris...\n",
              "Name: bomlink, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwjozotfjitG",
        "colab_type": "text"
      },
      "source": [
        "### Task 1.3 Fill number of critics for each rating film"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHPWz9TwdqmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count number of critics for each rating film\n",
        "def extractNumOfCritics(imdblink_):\n",
        "  url = imdblink_ + 'criticreviews'\n",
        "  response = requests.get(url)\n",
        "  # analyze rating website information\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "  # No score filter - simple count\n",
        "  filter_score = soup.findAll('div', {'class': [\"critscore critscore_favorable\", \n",
        "                           \"critscore critscore_mixed\", \n",
        "                           \"critscore critscore_unfavorable\"]})\n",
        "  return len(filter_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqOUjOPGCAC7",
        "colab_type": "code",
        "outputId": "b9b714d0-9edc-4144-922c-0940c666484b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "for i in range(0, film_merge.shape[0]):\n",
        "  film_merge['numberofcritics'][i] = extractNumOfCritics(film_merge['imdblink'][i])\n",
        "  if i%500 == 0:\n",
        "    print(i)\n",
        "film_merge['numberofcritics'].describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2376.000000\n",
              "mean        9.569865\n",
              "std         1.926043\n",
              "min         0.000000\n",
              "25%        10.000000\n",
              "50%        10.000000\n",
              "75%        10.000000\n",
              "max        10.000000\n",
              "Name: numberofcritics, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGpl7C0KCAAT",
        "colab_type": "code",
        "outputId": "6f856561-abc3-4f2f-f0a6-98def6514c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Number of films that lack critic reviews:\", len(film_merge[film_merge['numberofcritics'].isnull()]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of films that lack critic reviews: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc6Fz5OzL8Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write document\n",
        "film_merge.to_csv(\"Film-masterfile (from 'film-merge-8th.dta')-filled.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IYTmh7wLlWD",
        "colab_type": "text"
      },
      "source": [
        "## Task 2:\n",
        "(2) \"Film-cast-top5.csv\"\n",
        "Here you can add the following variable:\n",
        "- 'starpower-actor': StarMeter power of the top5 casts, measured 10 weeks prior to the release date of each film. \n",
        "\n",
        "You can use the variable 'releasedate' scraped from file (1).\n",
        "You can use 'actorimdb' to scrape variable 'starpower-actor'\n",
        "The given column is already created in the csv file at the right side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0sKeUQTdqjG",
        "colab_type": "code",
        "outputId": "2d53b49e-54a7-411b-ab4f-9b4dc270c375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "film_cast_top5 = pd.read_csv(\"Film-cast-top5.csv\")\n",
        "print(film_cast_top5.columns)\n",
        "film_cast_top5.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['bomfilmtitle', 'filmimdb', 'year', 'notsample', 'filmrank', 'castrank',\n",
            "       'filmrole', 'actorname', 'actorimdb', 'birthyear', 'gender', 'race',\n",
            "       'bomlink', 'starpower-actor'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bomfilmtitle</th>\n",
              "      <th>filmimdb</th>\n",
              "      <th>year</th>\n",
              "      <th>notsample</th>\n",
              "      <th>filmrank</th>\n",
              "      <th>castrank</th>\n",
              "      <th>filmrole</th>\n",
              "      <th>actorname</th>\n",
              "      <th>actorimdb</th>\n",
              "      <th>birthyear</th>\n",
              "      <th>gender</th>\n",
              "      <th>race</th>\n",
              "      <th>bomlink</th>\n",
              "      <th>starpower-actor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13 Going on 30</td>\n",
              "      <td>http://www.imdb.com/title/tt0337563/</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Cast</td>\n",
              "      <td>Jennifer Garner</td>\n",
              "      <td>http://www.imdb.com/name/nm0004950/</td>\n",
              "      <td>1972.0</td>\n",
              "      <td>f</td>\n",
              "      <td>white</td>\n",
              "      <td>http://www.boxofficemojo.com/movies/?id=13goin...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13 Going on 30</td>\n",
              "      <td>http://www.imdb.com/title/tt0337563/</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>Cast</td>\n",
              "      <td>Mark Ruffalo</td>\n",
              "      <td>http://www.imdb.com/name/nm0749263/</td>\n",
              "      <td>1967.0</td>\n",
              "      <td>m</td>\n",
              "      <td>white</td>\n",
              "      <td>http://www.boxofficemojo.com/movies/?id=13goin...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13 Going on 30</td>\n",
              "      <td>http://www.imdb.com/title/tt0337563/</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>Cast</td>\n",
              "      <td>Judy Greer</td>\n",
              "      <td>http://www.imdb.com/name/nm0339460/</td>\n",
              "      <td>1975.0</td>\n",
              "      <td>f</td>\n",
              "      <td>white</td>\n",
              "      <td>http://www.boxofficemojo.com/movies/?id=13goin...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     bomfilmtitle  ... starpower-actor\n",
              "0  13 Going on 30  ...             NaN\n",
              "1  13 Going on 30  ...             NaN\n",
              "2  13 Going on 30  ...             NaN\n",
              "\n",
              "[3 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}